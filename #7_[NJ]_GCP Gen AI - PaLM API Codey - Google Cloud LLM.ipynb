{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "YouTube Video here: https://www.youtube.com/watch?v=JmWASbSJqGY"
      ],
      "metadata": {
        "id": "AaMenHiwmqS_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HN5ByYVLl4b"
      },
      "source": [
        "# Vertex AI Codey API\n",
        "\n",
        "This notebook shows a few examples of using Codey via the latest Vertex AI SDK.\n",
        "\n",
        "What are LLMs\n",
        "Large language models (LLMs) are deep learning models trained on massive datasets of text. LLMs can translate language, summarize text, generate creative writing, generate code, power chatbots and virtual assistants, and complement search engines and recommendation systems. Creating an LLM requires massive amounts of data, significant compute resources, and specialized skills. Because LLMs require a big investment to create, they target broad rather than specific use cases. On Vertex AI, you can customize a foundation model for more specific tasks or knowledge domains by using prompt design and model tuning.\n",
        "The Vertex AI PaLM API enables you to test, customize, and deploy instances of Google’s large language models (LLM) so that you can leverage the capabilities of PaLM in your applications.\n",
        "PaLM 2 is Google's next generation Large Language Model (LLM) that builds on Google's legacy of breakthrough research in machine learning and responsible AI. PaLM 2 excels at tasks like advanced reasoning, translation, and code generation because of how it was built.\n",
        "Generative AI Studio and the Vertex AI PaLM API is powered by PaLM 2.\n",
        "Objectives:\n",
        "\n",
        "✅ Introduce Vertex PaLM API Text\n",
        "\n",
        "✅ Introduce Codey PaLM API components\n",
        "\n",
        "✅ Codey generated code samples\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHIXvyM6RHZJ"
      },
      "source": [
        "## Setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaOBG4Srn48k"
      },
      "source": [
        "Install the latest Vertex AI SDK for python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iP9JbFLbn8le"
      },
      "outputs": [],
      "source": [
        "!pip install google-cloud-aiplatform --upgrade --user"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twqcY0Uon-_Q"
      },
      "source": [
        "---\n",
        "\n",
        "#### ⚠️ Do not forget to click the \"RESTART RUNTIME\" button above.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3jBn1Zen2Xz"
      },
      "source": [
        "First let's set the GCP Project ID and use this to initialize the Vertex AI SDK. Please enter the project id and location."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth as google_auth\n",
        "google_auth.authenticate_user()"
      ],
      "metadata": {
        "id": "3Ds3MYQtaMEX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8amN4OqML-0a"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"<Enter your project id>\" #@param {type:\"string\"}\n",
        "LOCATION = \"us-central1\" #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l14vMsnNZ4B2"
      },
      "source": [
        "## Import the required libararies and initialize the sdk using project and location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIbFPdzxZ4B2",
        "outputId": "a2414850-5026-490d-8cf4-4c933fe20bb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vertex AI SDK version: 1.30.1 initialized!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.cloud import aiplatform\n",
        "from vertexai.language_models._language_models import TextGenerationModel, ChatModel, CodeChatModel, CodeGenerationModel\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
        "print(f\"Vertex AI SDK version: {aiplatform.__version__} initialized!\")\n",
        "\n",
        "from vertexai.language_models._language_models import CodeGenerationModel\n",
        "import IPython.display as display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q99xuHiYoI11"
      },
      "source": [
        "## Let's use codey to generate code!\n",
        "\n",
        "---\n",
        "\n",
        "#### ⚠️ You are using the model - code-bison@001.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lYXq1MvXo24i"
      },
      "outputs": [],
      "source": [
        "PROMPT = \"write a python function to find a leap year\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "0acE2b11oB0W",
        "outputId": "89be2b2a-fa18-434c-c641-a019e8cbc63a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```\ndef is_leap_year(year):\n  \"\"\"\n  Determines whether a year is a leap year.\n\n  Args:\n    year: The year to check.\n\n  Returns:\n    True if the year is a leap year, False otherwise.\n  \"\"\"\n\n  # A year is a leap year if it is divisible by 4, unless it is divisible by 100\n  # unless it is also divisible by 400.\n\n  if year % 4 == 0:\n    if year % 100 == 0:\n      return year % 400 == 0\n    else:\n      return True\n  else:\n    return False\n```"
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from vertexai.language_models._language_models import CodeGenerationModel\n",
        "import IPython.display as display\n",
        "\n",
        "codegen = CodeGenerationModel.from_pretrained(\"code-bison@001\")\n",
        "\n",
        "result = codegen.predict(\n",
        "    PROMPT,\n",
        "    # Optional:\n",
        "    max_output_tokens=528,\n",
        "    temperature=0.65,\n",
        "    #top_p=1,\n",
        "    #top_k=5,\n",
        ")\n",
        "\n",
        "display.Markdown(result.text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQPeMz92WKag"
      },
      "source": [
        "## Let's use codey to write a unit test for a function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCbUrcSRWL8n",
        "outputId": "9ebc77c0-70e6-4b6c-a197-dcd749f106fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "def test_is_leap_year():\n",
            "  \"\"\"Test for the function is_leap_year.\"\"\"\n",
            "\n",
            "  # Check that the function returns True for years that are divisible by 400.\n",
            "  assert is_leap_year(400) == True\n",
            "\n",
            "  # Check that the function returns True for years that are divisible by 100 but not by 400.\n",
            "  assert is_leap_year(100) == False\n",
            "\n",
            "  # Check that the function returns True for years that are divisible by 4 but not by 100.\n",
            "  assert is_leap_year(4) == True\n",
            "\n",
            "  # Check that the function returns False for years that are not divisible by 4.\n",
            "  assert is_leap_year(3) == False\n",
            "\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "PROMPT = \"\"\"Write a unit test for this function\n",
        "def is_leap_year(year):\n",
        "  if year % 4 == 0:\n",
        "    if year % 100 == 0:\n",
        "      if year % 400 == 0:\n",
        "        return True\n",
        "      else:\n",
        "        return False\n",
        "    else:\n",
        "      return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "response = codegen.predict(\n",
        "    PROMPT,\n",
        "    # Optional:\n",
        "    max_output_tokens=654,\n",
        "    temperature=0.65,\n",
        "    #top_p=1,\n",
        "    #top_k=5,\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hd86dwD3UHBv"
      },
      "source": [
        "## Another Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "id": "bKl4jBnMUGmW",
        "outputId": "3c7d6d9c-c60e-4ae1-e71e-56175faad5ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"log\"\n    \"net/http\"\n)\n\n// HandlerFunc defines the handler function type.\ntype HandlerFunc func(http.ResponseWriter, *http.Request)\n\n// ServeHTTP implements the http.Handler interface.\nfunc (h HandlerFunc) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n    h(w, r)\n}\n\n// HelloHandler is the HTTP handler that greets the user.\nfunc HelloHandler(w http.ResponseWriter, r *http.Request) {\n    // Decode the JSON message.\n    var name struct {\n        Name string `json:\"name\"`\n    }\n    if err := json.NewDecoder(r.Body).Decode(&name); err != nil {\n        log.Fatal(err)\n    }\n\n    // Prepend \"hello \" to the name and return it.\n    fmt.Fprintf(w, \"hello %s\\n\", name.Name)\n}\n\nfunc main() {\n    // Create a new HTTP server.\n    http.HandleFunc(\"/hello\", HelloHandler)\n    server := http.Server{Addr: \":8080\"}\n\n    // Start the HTTP server.\n    if err := server.ListenAndServe(); err != nil {\n        log.Fatal(err)\n    }\n}\n```"
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "PROMPT = \"\"\"Write an http server in Go that takes a JSON message as a POST, extracts the name, and returns it, prepending \"hello \".\n",
        "Make sure the HTTP handler is in a separate method.\n",
        "Here is an example JSON:\n",
        "\n",
        "{\"name\":\"squirrel\"}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "response = codegen.predict(\n",
        "    PROMPT,\n",
        "    # Optional:\n",
        "    max_output_tokens=654,\n",
        "    temperature=0.65,\n",
        "    #top_p=1,\n",
        "    #top_k=5,\n",
        ")\n",
        "\n",
        "\n",
        "display.Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeN2xhSRcyft"
      },
      "source": [
        "## Let's try chat codey to generate code!\n",
        "\n",
        "---\n",
        "\n",
        "#### ⚠️ You are using the model - codechat-bison@001.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_2GRbQ7c4Hq",
        "outputId": "bbb9df79-e392-4bdd-8b31-febe05f41858"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from Model: I am doing well, thank you for asking! I am excited to be learning more about natural language processing and how to use it to help people.\n"
          ]
        }
      ],
      "source": [
        "import vertexai\n",
        "from vertexai.preview.language_models import CodeChatModel\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n",
        "chat_model = CodeChatModel.from_pretrained(\"codechat-bison@001\")\n",
        "parameters = {\n",
        "    \"temperature\": 0.2,\n",
        "    \"max_output_tokens\": 1024\n",
        "}\n",
        "\n",
        "\n",
        "chat = chat_model.start_chat()\n",
        "response = chat.send_message(\"\"\"Hi, how are you?\"\"\", **parameters)\n",
        "print(f\"Response from Model: {response.text}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "teJDEe3zEzWW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40456349-6ce4-4e82-908c-ad28c1203304"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from Model: There are many things you can help me with in the coding world! For example, you can help me learn new programming languages, debug my code, and give me feedback on my projects.\n"
          ]
        }
      ],
      "source": [
        "response = chat.send_message(\"\"\"I am doing good. What Can I help you with in the coding world?\"\"\", **parameters)\n",
        "print(f\"Response from Model: {response.text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Lem4SVXZ4B5",
        "outputId": "ec79cc5a-5548-48c4-afd6-b9edd5abae94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from Model: Sure, here is a function to calculate the minimum of two numbers:\n",
            "\n",
            "```\n",
            "def min(a, b):\n",
            "  \"\"\"\n",
            "  Calculates the minimum of two numbers.\n",
            "\n",
            "  Args:\n",
            "    a: The first number.\n",
            "    b: The second number.\n",
            "\n",
            "  Returns:\n",
            "    The smaller of the two numbers.\n",
            "  \"\"\"\n",
            "\n",
            "  if a < b:\n",
            "    return a\n",
            "  else:\n",
            "    return b\n",
            "```\n",
            "\n",
            "This function takes two numbers as input and returns the smaller of the two numbers.\n"
          ]
        }
      ],
      "source": [
        "response = chat.send_message(\"\"\"Please help write a function to calculate the min of two numbers\"\"\", **parameters)\n",
        "print(f\"Response from Model: {response.text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWTyrHpEZ4B5",
        "outputId": "8fa82357-e955-4ebf-a710-00afdd1bf50b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from Model: Sure, here is the function with the variable names changed:\n",
            "\n",
            "```\n",
            "def min(first_num, second_num):\n",
            "  \"\"\"\n",
            "  Calculates the minimum of two numbers.\n",
            "\n",
            "  Args:\n",
            "    first_num: The first number.\n",
            "    second_num: The second number.\n",
            "\n",
            "  Returns:\n",
            "    The smaller of the two numbers.\n",
            "  \"\"\"\n",
            "\n",
            "  if first_num < second_num:\n",
            "    return first_num\n",
            "  else:\n",
            "    return second_num\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "response = chat.send_message(\"\"\"Please change the previous function variables from a to first_num and b to second_num\"\"\", **parameters)\n",
        "print(f\"Response from Model: {response.text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wedn1zwCCUM_"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "environment": {
      "kernel": "python3",
      "name": "tf2-gpu.2-11.m110",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m110"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
