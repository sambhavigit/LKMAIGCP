{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sambhavigit/LKMAIGCP/blob/main/Batch4/text_to_video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr2jRuqabG1m"
      },
      "source": [
        "# Vertex AI Model Garden - Text To Video\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "Colab Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLLfRT_6bTZO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates deploying the pre-trained [Text To Video](https://huggingface.co/docs/diffusers/main/en/api/pipelines/text_to_video) model on Vertex AI for online prediction.\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Upload the model to [Model Registry](https://cloud.google.com/vertex-ai/docs/model-registry/introduction).\n",
        "- Deploy the model on [Endpoint](https://cloud.google.com/vertex-ai/docs/predictions/using-private-endpoints).\n",
        "- Run online predictions for text-to-video.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsQlA1Yibroo"
      },
      "outputs": [],
      "source": [
        "if \"google.colab\" in str(get_ipython()):\n",
        "    ! pip3 install --upgrade google-cloud-aiplatform\n",
        "\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)\n",
        "\n",
        "    from google.colab import auth as google_auth\n",
        "\n",
        "    google_auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okVArp5bb3Wu"
      },
      "outputs": [],
      "source": [
        "# Cloud project id.\n",
        "PROJECT_ID = \"acn-lkmaigcp\"  # @param {type:\"string\"}\n",
        "\n",
        "# The region you want to launch jobs in.\n",
        "REGION = \"us-east1\"  # @param {type:\"string\"}\n",
        "\n",
        "# The Cloud Storage bucket for storing experiments output. Fill it without the 'gs://' prefix.\n",
        "GCS_BUCKET = \"\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92f16e22c20b"
      },
      "source": [
        "Initialize Vertex AI API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1680c257acfb"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=\"acn-lkmaigcp\", location=\"us-east1\"  , staging_bucket=GCS_BUCKET)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ca48b699d17"
      },
      "source": [
        "### Define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de9882ea89ea"
      },
      "outputs": [],
      "source": [
        "# The pre-built serving docker image. It contains serving scripts and models.\n",
        "SERVE_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-diffusers-serve\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cac4478ae098"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "\n",
        "def deploy_model(model_id, task):\n",
        "    model_name = \"text-to-video\"\n",
        "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
        "    serving_env = {\n",
        "        \"MODEL_ID\": model_id,\n",
        "        \"TASK\": task,\n",
        "    }\n",
        "    # If the model_id is a GCS path, use artifact_uri to pass it to serving docker.\n",
        "    artifact_uri = model_id if model_id.startswith(\"gs://\") else None\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=SERVE_DOCKER_URI,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=\"/predictions/diffusers_serving\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "        serving_container_environment_variables=serving_env,\n",
        "        artifact_uri=artifact_uri,\n",
        "    )\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=\"n1-standard-8\",\n",
        "        accelerator_type=\"NVIDIA_TESLA_T4\",\n",
        "        accelerator_count=1,\n",
        "        deploy_request_timeout=1800,\n",
        "    )\n",
        "    return model, endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4b46c28d8b1"
      },
      "outputs": [],
      "source": [
        "##Creating Endpoint and Model Registration\n",
        "\n",
        "model, endpoint = deploy_model(\n",
        "    model_id=\"damo-vilab/text-to-video-ms-1.7b\", task=\"text-to-video\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6be655247cb1"
      },
      "outputs": [],
      "source": [
        "##Prompt\n",
        "\n",
        "instances = [\n",
        "    {\"prompt\": \"Spiderman and batman is surfing\"},\n",
        "]\n",
        "response = endpoint.predict(instances=instances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nriQMwuOiS0k"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "html = \"\"\n",
        "for video in response.predictions:\n",
        "    html += \"<video controls>\"\n",
        "    html += f'<source src=\"data:video/mp4;base64,{video}\" type=\"video/mp4\">'\n",
        "    html += \"</video>\"\n",
        "HTML(html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db7ffebdb4be"
      },
      "source": [
        "### Clean up resources if not using this model further for any video creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ccf3714dbe9"
      },
      "outputs": [],
      "source": [
        "# Undeploy model and delete endpoint.\n",
        "endpoint.delete(force=True)\n",
        "\n",
        "# Delete models.\n",
        "model.delete()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_pytorch_text_to_video.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}